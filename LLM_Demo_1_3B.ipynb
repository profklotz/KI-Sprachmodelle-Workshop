{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LLM Demo 1.3B",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vorbereitung\n",
        "Bevor wir mit der Nutzung des Modells starten können, müssen wir erst zwei wichtige Python-Pakete installieren. Wir nutzen dafür den Python-Paketmanager `pip3`. Zum einen installieren wir das Paket [PyTorch](https://pytorch.org/), ein recht populäres Framework für Machine Learning Anwendungen, und zum anderen das Paket [Transformers](https://github.com/huggingface/transformers), welches von der KI-as-a-Service Plattform [Huggingface](https://huggingface.co/) bereitgestellt wird und die Nutzung vieler Verfügbarer, vortrainierter KI-Modelle stark vereinfacht."
      ],
      "metadata": {
        "id": "AivgauljwqOh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k43Aolzk6Wjz"
      },
      "outputs": [],
      "source": [
        "!pip3 install torch\n",
        "!pip3 install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Das eigentliche Programm\n",
        "## Import der notwendigen Pakete\n",
        "Jetzt können wir mit dem eigentlichen Programmieren loslegen. Als erstes importieren wir das zuvor installiere Transformers-Paket, mit dem wir denkbar einfach auf die verfügbaren KI-Modelle von Huggingface zugreifen können. "
      ],
      "metadata": {
        "id": "xzS2UR0u06vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "K28yF6edyNkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zugriff auf bestehende KI-Modelle\n",
        "Der nächste Befehl nutzt die `pipeline` Schnittstelle, um ein verfügbares, vortrainiertes KI-Modell in unserem Programm zu verwenden. Mit dem ersten Parameter `'text-generation'` teilen wir der Schnittstelle mit, für welchen Anwendungsfall wir das Modell verwenden möchten -- in diesem Beispiel die Erzeugung von Text. Der zweite Parameter `'EleutherAI/gpt-neo-1.3B'` legt das Modell fest, welches wir nutzen möchten. Wir nutzen in diesem Beispiel einen [Open Source Klon](https://huggingface.co/EleutherAI/gpt-neo-1.3B) des sehr bekannten Modells `GPT-3`, welches jedoch deutlich kleiner ist (damit wir das kostenlose Angebot von Google Colab verwenden können).\n",
        "\n",
        "Es gibt auf Huggingface noch [viele weitere vortrainierte Modelle](https://huggingface.co/models), die teilweise auch deutlich umfangreicher und damit leistungsfähiger sind.\n",
        "\n",
        "Dank des Transformers-Pakets brauchen wir uns als Entwickler nicht mit technischen Details herumschlagen. Stattdessen erkennt Transformers bei der ersten Ausführung der Zelle automatisch, dass das Modell nocht nicht in dieser Umgebung vorhanden ist und lädt es herunter (Größe: ca. 5 GB, das kann wenige Minuten dauern). "
      ],
      "metadata": {
        "id": "WPGm4-r-1UTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')"
      ],
      "metadata": {
        "id": "jIqNvDKVya6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Der Aufruf des KI-Modells\n",
        "Der eigentliche Aufruf des Modells ist dann -- Dank des Transformer-Frameworks -- erneut recht einfach. Wir übergeben lediglich einen String mit dem Beginn des Textes und das Modell wird dann diesen Text für uns fortsetzen. Am Ende geben wir das Ergebnis aus.\n",
        "\n",
        "Hinweis: Das Modell ist nicht-deterministisch, das heißt bei mehrfacher Ausführung der Zelle werden unterschiedliche Texte generiert."
      ],
      "metadata": {
        "id": "9e3ZNCES4ajB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Der Text, den das Modell für uns fortsetzen soll:\n",
        "prompt = \"For insurances, AI will be\"\n",
        "\n",
        "# Das Modell aufrufen für die Fortführung des Textes\n",
        "result = generator(prompt, do_sample=True, min_length=50)\n",
        "\n",
        "# Das Ergebnis anzeigen\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "L7C2MLgc0GX3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}